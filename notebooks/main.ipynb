{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- iPython Config --- #\n",
    "from IPython import get_ipython\n",
    "if 'IPython.extensions.autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "else:\n",
    "    get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "%autoreload 2\n",
    "\n",
    "# --- System and Path --- #\n",
    "import os\n",
    "import sys\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# --- Standard Libraries --- #\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Dict\n",
    "import time\n",
    "import joblib\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "# --- Custom Modules --- #\n",
    "from src.config import Config\n",
    "from src.data import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: Before=134.49MB -> After=65.62MB, Decreased by 51.2%\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(repo_path, \"data\", \"raw\", \"dataset.parquet\")\n",
    "df_dataset = dataloader.load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing datasets, including splitting and normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor class with default attributes.\n",
    "        \"\"\"\n",
    "        self.df_dataset: Optional[pd.DataFrame] = None\n",
    "        self.target: Optional[str] = None\n",
    "        self.random_state: int = Config.SEED  # Assumes Config.SEED is defined globally\n",
    "\n",
    "    def initial_train_test_split(self, df_dataset: pd.DataFrame, test_size: float = 0.10) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and test sets.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataset.\n",
    "            test_size (float, optional): Proportion of the dataset to be used as the test set. Defaults to 0.10.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Training and test datasets.\n",
    "        \"\"\"\n",
    "        if not self.target:\n",
    "            raise ValueError(\"Target column must be specified before splitting.\")\n",
    "\n",
    "        X = df_dataset.drop(columns=[self.target])\n",
    "        y = df_dataset[self.target]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        # combine X and y\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "    def normalize(self, df_train: pd.DataFrame, df_test: pd.DataFrame, num_cols: Optional[list] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Normalizes numerical features in the dataset using StandardScaler.\n",
    "\n",
    "        Args:\n",
    "            df_train (pd.DataFrame): Training dataset.\n",
    "            df_test (pd.DataFrame): Test dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Normalized training and test datasets.\n",
    "        \"\"\"\n",
    "        if num_cols is None:\n",
    "            num_cols = df_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        if self.target in num_cols:\n",
    "            num_cols.remove(self.target)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n",
    "        df_test[num_cols] = scaler.transform(df_test[num_cols])\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "\n",
    "    def process(self, df_dataset: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Processes the dataset by splitting into train and test sets.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataset to be processed.\n",
    "            target (str): The target column for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Processed train and test datasets.\n",
    "        \"\"\"\n",
    "        if not target:\n",
    "            raise ValueError(\"Target column must be specified.\")\n",
    "\n",
    "        self.df_dataset = df_dataset\n",
    "        self.target = target\n",
    "\n",
    "        # Initial split: dataset â†’ train (80+10%) | test (10%)\n",
    "        df_train, df_test = self.initial_train_test_split(self.df_dataset, test_size=0.10)\n",
    "\n",
    "        # Normalize\n",
    "        df_train, df_test = self.normalize(df_train, df_test)\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "data_processor = DataProcessor()\n",
    "df_train, df_test = data_processor.process(df_dataset, target=\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.params = {}\n",
    "        self.metrics = {\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"roc_auc\": roc_auc_score\n",
    "        } # {metric_name: metric_function}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y) -> dict:\n",
    "        y_pred = self.predict(X)\n",
    "        results = {}\n",
    "        for metric_name, metric_fn in self.metrics.items():\n",
    "            try:\n",
    "                if metric_name == \"roc_auc\":\n",
    "                    y_prob = self.model.predict_proba(X)[:, 1]  # Get probabilities for the positive class (1)\n",
    "                    results[metric_name] = metric_fn(y, y_prob)\n",
    "                else:\n",
    "                    results[metric_name] = metric_fn(y, y_pred)\n",
    "            except ValueError:\n",
    "                results[metric_name] = None\n",
    "        return results\n",
    "\n",
    "class LogisticRegressionModel(BaseModel):\n",
    "    def __init__(self, random_state=Config.SEED):\n",
    "        super().__init__()\n",
    "        self.model = LogisticRegression()\n",
    "        self.base_params = {\"random_state\": random_state}\n",
    "        self.learnable_params = {\n",
    "            \"C\": {\"type\": \"loguniform\", \"low\": 0.01, \"high\": 10}  # Regularization strength\n",
    "        }\n",
    "        self.params = {**self.base_params}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.set_params(**self.params)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "\n",
    "class XGBoostModel(BaseModel):\n",
    "    def __init__(self, random_state=Config.SEED):\n",
    "        super().__init__()\n",
    "        self.model = xgb.XGBClassifier()\n",
    "        self.base_params = {\"random_state\": random_state, \"objective\": \"binary:logistic\"}\n",
    "        self.learnable_params = {\n",
    "            \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 10},\n",
    "            \"learning_rate\": {\"type\": \"loguniform\", \"low\": 0.01, \"high\": 0.3},\n",
    "            \"n_estimators\": {\"type\": \"int\", \"low\": 50, \"high\": 300}\n",
    "        }\n",
    "        self.params = {**self.base_params}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.set_params(**self.params)\n",
    "        self.model.fit(np.array(X), np.array(y))\n",
    "\n",
    "class SingleTrainer:\n",
    "    def __init__(self, model:BaseModel):\n",
    "        self.model = model # Custom model instance (e.g., LogisticRegressionModel)\n",
    "        self.target = \"\"\n",
    "\n",
    "    def train(self, df_train, target, tune_params=False):\n",
    "        self.target = target # update target\n",
    "        X = df_train.drop(columns=[self.target]).values\n",
    "        y = df_train[self.target].values\n",
    "\n",
    "        if tune_params:\n",
    "            best_params = self.tune_hyperparameters(df_train)\n",
    "            self.model.model.set_params(**best_params)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def _retrieve_search_space(self, model, trial):\n",
    "            \"\"\"Retrieve and suggest hyperparameter search space for Optuna tuning.\"\"\"\n",
    "            trial_params = {}\n",
    "            for param, search_space in model.learnable_params.items():\n",
    "                if search_space[\"type\"] == \"int\":\n",
    "                    trial_params[param] = trial.suggest_int(param, search_space[\"low\"], search_space[\"high\"])\n",
    "                elif search_space[\"type\"] == \"loguniform\":\n",
    "                    trial_params[param] = trial.suggest_loguniform(param, search_space[\"low\"], search_space[\"high\"])\n",
    "            return trial_params\n",
    "\n",
    "    def _cross_validate(self, X, y, params):\n",
    "        \"\"\"Perform cross-validation and return fold scores\"\"\"\n",
    "        n_splits = 5\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=Config.SEED)\n",
    "        fold_scores = []\n",
    "        for train_index, val_index in cv.split(X, y):\n",
    "            x_tr, x_val = X[train_index], X[val_index]\n",
    "            y_tr, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            model = type(self.model)()  # Re-instantiate model\n",
    "            model.model.set_params(**params)\n",
    "            model.fit(x_tr, y_tr)\n",
    "            score = model.evaluate(x_val, y_val)[self.main_metric]\n",
    "            fold_scores.append(score)\n",
    "        return fold_scores\n",
    "\n",
    "    def objective(self, trial, df_train):\n",
    "        \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n",
    "        # Search space\n",
    "        trial_params = self._retrieve_search_space(self.model, trial)\n",
    "        # Cross-validation\n",
    "        X = df_train.drop(columns=[self.target]).values\n",
    "        y = df_train[self.target].values\n",
    "        fold_scores = self._cross_validate(X, y, trial_params)\n",
    "        return np.mean(fold_scores)\n",
    "\n",
    "    def tune_hyperparameters(self, df_train, n_trials=3) -> dict:\n",
    "        \"\"\"Optimize hyperparameters using Optuna\"\"\"\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(lambda trial: self.objective(trial, df_train), n_trials=n_trials)\n",
    "        return study.best_params\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, df_train, df_test, target:str, models: Dict[str, BaseModel], main_metric:str, verbose=True, output_dir:str=None):\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.target = target\n",
    "        self.models: Dict[str, BaseModel] = models # {model_name: model_instance}\n",
    "        self.trained_models: Dict[str, BaseModel] = {}\n",
    "        self.main_metric = main_metric\n",
    "        self.verbose = verbose\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_model(model:BaseModel, output_dir, file_format='pkl', verbose=True):\n",
    "        time_now = time.strftime(\"%Y-%m-%d-%H%M\")\n",
    "        model_name = model.__class__.__name__\n",
    "        file_name = f\"{time_now}-{model_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        file_path = os.path.join(output_dir, f\"{file_name}.{file_format}\")\n",
    "        joblib.dump(model, file_path)\n",
    "        if verbose:\n",
    "            print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    def train_all_models(self, tune_params:bool=False):\n",
    "        \"\"\"Train and tune all models\"\"\"\n",
    "        for model_name, model in self.models.items():\n",
    "            if self.verbose:\n",
    "                print(f\"Training {model_name}...\")\n",
    "                start_time = time.time()\n",
    "\n",
    "            single_trainer = SingleTrainer(model)\n",
    "            single_trainer.train(self.df_train, self.target, tune_params)\n",
    "            self.trained_models[model_name] = single_trainer.model\n",
    "            # Save model\n",
    "            if self.output_dir:\n",
    "                self._save_model(single_trainer.model, self.output_dir, verbose=self.verbose)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Training time {model_name}: {time.time()-start_time:.2f} seconds.\")\n",
    "\n",
    "    def evaluate_all_models(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Evaluate all trained models on both train and test sets.\"\"\"\n",
    "        results = {\"train\": {}, \"test\": {}}\n",
    "\n",
    "        for dataset_name, df in [(\"train\", self.df_train), (\"test\", self.df_test)]:\n",
    "            X = df.drop(columns=[self.target]).values\n",
    "            y = df[self.target].values\n",
    "\n",
    "            for model_name, model in self.trained_models.items():\n",
    "                res = model.evaluate(X, y)\n",
    "                score = res.get(self.main_metric, None)  # Avoid KeyError\n",
    "                results[dataset_name][model_name] = res\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        f\"{dataset_name.upper()} | {model_name} {self.main_metric}: {score:.4f}\"\n",
    "                        if score is not None\n",
    "                        else f\"{dataset_name.upper()} | {model_name}: Metric not available\"\n",
    "                    )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "Model saved to /Users/pupipatsingkhorn/Developer/repositories/fraud-detection-european-credit-card-transactions-2023/models/2025-02-16-2158-LogisticRegressionModel.pkl\n",
      "Training time LogisticRegression: 0.87 seconds.\n",
      "Training XGBoost...\n",
      "Model saved to /Users/pupipatsingkhorn/Developer/repositories/fraud-detection-european-credit-card-transactions-2023/models/2025-02-16-2158-XGBoostModel.pkl\n",
      "Training time XGBoost: 1.51 seconds.\n",
      "TRAIN | LogisticRegression recall: 0.9977\n",
      "TRAIN | XGBoost recall: 1.0000\n",
      "TEST | LogisticRegression recall: 0.9976\n",
      "TEST | XGBoost recall: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': {'LogisticRegression': {'precision': 0.9991272391403893,\n",
       "   'recall': 0.9976708195901268,\n",
       "   'f1': 0.9983984982254421,\n",
       "   'accuracy': 0.9983996623463413,\n",
       "   'roc_auc': 0.9998447527758879},\n",
       "  'XGBoost': {'precision': 1.0,\n",
       "   'recall': 1.0,\n",
       "   'f1': 1.0,\n",
       "   'accuracy': 1.0,\n",
       "   'roc_auc': 1.0}},\n",
       " 'test': {'LogisticRegression': {'precision': 0.999154572354516,\n",
       "   'recall': 0.9976434173965039,\n",
       "   'f1': 0.9983984230627079,\n",
       "   'accuracy': 0.9983996623463413,\n",
       "   'roc_auc': 0.9998275054532307},\n",
       "  'XGBoost': {'precision': 0.9998592837543094,\n",
       "   'recall': 0.9996834441278886,\n",
       "   'f1': 0.9997713562094377,\n",
       "   'accuracy': 0.9997713803351916,\n",
       "   'roc_auc': 0.9999653776293036}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "models = {\"LogisticRegression\": LogisticRegressionModel(), \"XGBoost\": XGBoostModel()}\n",
    "# Initialize trainer\n",
    "multi_trainer = Trainer(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    target=\"Class\",\n",
    "    models=models,\n",
    "    main_metric=\"recall\",\n",
    "    output_dir=os.path.join(repo_path, \"models\"),\n",
    ")\n",
    "multi_trainer.train_all_models(tune_params=False)\n",
    "multi_trainer.evaluate_all_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
