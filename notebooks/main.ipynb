{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- iPython Config --- #\n",
    "from IPython import get_ipython\n",
    "if 'IPython.extensions.autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "else:\n",
    "    get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "%autoreload 2\n",
    "\n",
    "# --- System and Path --- #\n",
    "import os\n",
    "import sys\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# --- Standard Libraries --- #\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "# --- Custom Modules --- #\n",
    "from src.config import Config\n",
    "from src.data import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: Before=134.49MB -> After=65.62MB, Decreased by 51.2%\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(repo_path, \"data\", \"raw\", \"dataset.parquet\")\n",
    "df_dataset = dataloader.load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing datasets, including splitting and normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor class with default attributes.\n",
    "        \"\"\"\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.target: Optional[str] = None\n",
    "        self.random_state: int = Config.SEED  # Assumes Config.SEED is defined globally\n",
    "\n",
    "    def initial_train_test_split(self, df: pd.DataFrame, test_size: float = 0.10) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and test sets.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataset.\n",
    "            test_size (float, optional): Proportion of the dataset to be used as the test set. Defaults to 0.10.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Training and test datasets.\n",
    "        \"\"\"\n",
    "        if not self.target:\n",
    "            raise ValueError(\"Target column must be specified before splitting.\")\n",
    "\n",
    "        X = df.drop(columns=[self.target])\n",
    "        y = df[self.target]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        # combine X and y\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "    def normalize(self, df_train: pd.DataFrame, df_test: pd.DataFrame, num_cols: Optional[list] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Normalizes numerical features in the dataset using StandardScaler.\n",
    "\n",
    "        Args:\n",
    "            df_train (pd.DataFrame): Training dataset.\n",
    "            df_test (pd.DataFrame): Test dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Normalized training and test datasets.\n",
    "        \"\"\"\n",
    "        if num_cols is None:\n",
    "            num_cols = df_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        if self.target in num_cols:\n",
    "            num_cols.remove(self.target)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n",
    "        df_test[num_cols] = scaler.transform(df_test[num_cols])\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "\n",
    "    def process(self, df: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Processes the dataset by splitting into train and test sets.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataset to be processed.\n",
    "            target (str): The target column for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Processed train and test datasets.\n",
    "        \"\"\"\n",
    "        if not target:\n",
    "            raise ValueError(\"Target column must be specified.\")\n",
    "\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "\n",
    "        # Initial split: dataset â†’ train (80+10%) | test (10%)\n",
    "        df_train, df_test = self.initial_train_test_split(self.df, test_size=0.10)\n",
    "\n",
    "        # Normalize numerical features\n",
    "        df_train, df_test = self.normalize(df_train, df_test)\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "# Example usage\n",
    "data_processor = DataProcessor()\n",
    "df_train, df_test = data_processor.process(df_dataset, target=\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.params = {}\n",
    "        self.metrics = {\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"roc_auc\": roc_auc_score\n",
    "        } # {metric_name: metric_function}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y) -> dict:\n",
    "        y_pred = self.predict(X)\n",
    "        results = {}\n",
    "        for metric_name, metric_fn in self.metrics.items():\n",
    "            try:\n",
    "                if metric_name == \"roc_auc\":\n",
    "                    pos_class = 1\n",
    "                    y_prob = self.model.predict_proba(X)[:, pos_class]  # Get probabilities for the positive class\n",
    "                    results[\"roc_auc\"] = metric_fn(y, y_prob)\n",
    "                else:\n",
    "                    results[metric_name] = metric_fn(y, y_pred)\n",
    "            except ValueError:\n",
    "                results[metric_name] = None\n",
    "        return results\n",
    "\n",
    "class LogisticRegressionModel(BaseModel):\n",
    "    def __init__(self, random_state=Config.SEED):\n",
    "        super().__init__()\n",
    "        self.model = LogisticRegression()\n",
    "        self.base_params = {\"random_state\": random_state}\n",
    "        self.learnable_params = {\n",
    "            \"C\": {\"type\": \"loguniform\", \"low\": 0.01, \"high\": 10}  # Regularization strength\n",
    "        }\n",
    "        self.params = {**self.base_params}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.set_params(**self.params)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "\n",
    "class XGBoostModel(BaseModel):\n",
    "    def __init__(self, random_state=Config.SEED):\n",
    "        super().__init__()\n",
    "        self.model = xgb.XGBClassifier()\n",
    "        self.base_params = {\"random_state\": random_state, \"objective\": \"binary:logistic\"}\n",
    "        self.learnable_params = {\n",
    "            \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 10},\n",
    "            \"learning_rate\": {\"type\": \"loguniform\", \"low\": 0.01, \"high\": 0.3},\n",
    "            \"n_estimators\": {\"type\": \"int\", \"low\": 50, \"high\": 300}\n",
    "        }\n",
    "        self.params = {**self.base_params}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.set_params(**self.params)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "class SingleTrainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.target = \"\"\n",
    "\n",
    "    def train(self, df_train, target, tune_params=False):\n",
    "        self.target = target # update target\n",
    "        X = df_train.drop(columns=[self.target]).values\n",
    "        y = df_train[self.target].values\n",
    "\n",
    "        if tune_params:\n",
    "            best_params = self.tune_hyperparameters(df_train)\n",
    "            self.model.model.set_params(**best_params)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def objective(self, trial, df_train):\n",
    "        \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n",
    "        # Define hyperparameter search space\n",
    "        trial_params = {}\n",
    "        def _retrieve_search_space(model):\n",
    "            trial_params = {}\n",
    "            for param, search_space in model.learnable_params.items():\n",
    "                if search_space[\"type\"] == \"int\":\n",
    "                    trial_params[param] = trial.suggest_int(param, search_space[\"low\"], search_space[\"high\"])\n",
    "                elif search_space[\"type\"] == \"loguniform\":\n",
    "                    trial_params[param] = trial.suggest_loguniform(param, search_space[\"low\"], search_space[\"high\"])\n",
    "            return trial_params\n",
    "        trial_params = _retrieve_search_space(self.model)\n",
    "\n",
    "        # Cross-validation\n",
    "        X = df_train.drop(columns=[self.target]).values\n",
    "        y = df_train[self.target].values\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.SEED)\n",
    "        fold_scores = []\n",
    "        for train_index, val_index in cv.split(X, y):\n",
    "            x_tr, x_val = X[train_index], X[val_index]\n",
    "            y_tr, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            model = type(self.model)()  # Re-instantiate model\n",
    "            model.model.set_params(**trial_params)\n",
    "            model.fit(x_tr, y_tr)\n",
    "            score = model.evaluate(x_val, y_val)[self.main_metric]\n",
    "            fold_scores.append(score)\n",
    "\n",
    "        return np.mean(fold_scores)\n",
    "\n",
    "    def tune_hyperparameters(self, df_train, n_trials=3) -> dict:\n",
    "        \"\"\"Optimize hyperparameters using Optuna\"\"\"\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(lambda trial: self.objective(trial, df_train), n_trials=n_trials)\n",
    "        return study.best_params\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, df_train, df_test, target, models: dict):\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.target = target\n",
    "        self.models = models # {model_name: model_instance}\n",
    "        self.trained_models = {}\n",
    "        self.main_metric = \"recall\"\n",
    "\n",
    "    def train_all_models(self, tune_params:bool=False):\n",
    "        \"\"\"Train and tune all models\"\"\"\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Training {model_name}...\")\n",
    "            single_trainer = SingleTrainer(model)\n",
    "            single_trainer.train(self.df_train, self.target, tune_params)\n",
    "            self.trained_models[model_name] = single_trainer.model\n",
    "\n",
    "    def evaluate_all_models(self):\n",
    "        \"\"\"Evaluate all trained models\"\"\"\n",
    "        X_test = self.df_test.drop(columns=[self.target]).values\n",
    "        y_test = self.df_test[self.target].values\n",
    "        results = {}\n",
    "        for model_name, model in self.trained_models.items():\n",
    "            res = model.evaluate(X_test, y_test)\n",
    "            print(res)\n",
    "            score = res[self.main_metric]\n",
    "            results[model_name] = score\n",
    "            print(f\"{model_name} {self.main_metric}: {score:.4f}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "Training XGBoost...\n",
      "{'precision': 0.999154572354516, 'recall': 0.9976434173965039, 'f1_score': 0.9983984230627079, 'accuracy': 0.9983996623463413, 'roc_auc': 0.9998275054532307}\n",
      "LogisticRegression recall: 0.9976\n",
      "{'precision': 0.9998592837543094, 'recall': 0.9996834441278886, 'f1_score': 0.9997713562094377, 'accuracy': 0.9997713803351916, 'roc_auc': 0.9999653776293036}\n",
      "XGBoost recall: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': 0.9976434173965039, 'XGBoost': 0.9996834441278886}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train All Models with Hyperparameter Tuning\n",
    "models = {\"LogisticRegression\": LogisticRegressionModel(), \"XGBoost\": XGBoostModel()}\n",
    "multi_trainer = Trainer(df_train, df_test, target=\"Class\", models=models)\n",
    "multi_trainer.train_all_models(tune_params=False)\n",
    "multi_trainer.evaluate_all_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
